# # In Emacs org-mode: before exporting, comment this out START
# ;; Local Variables:
# ;; ispell-check-comments: exclusive
# ;; ispell-local-dictionary: "english"
# ;; End:
# # In Emacs org-mode: before exporting, comment this out FINISH

# Org-mode Export LaTeX Customization Notes:
# - Interpret 'bla_bla' as LaTeX Math bla subscript bla: #+OPTIONS ^:t. Interpret literally bla_bla: ^:nil.
# - org export: turn off heading -> section numbering: #+OPTIONS: num:nil
# - org export: change list numbering to alphabetical, sources:
#   - https://orgmode.org/manual/Plain-lists-in-LaTeX-export.html
#   - https://tex.stackexchange.com/a/129960
#   - must be inserted before each list:
#     #+ATTR_LATEX: :environment enumerate
#     #+ATTR_LATEX: :options [label=\alph*)]
# - allow org to recognize alphabetical lists a)...: M-x customize-variable org-list-allow-alphabetical


# -----------------------
# General Export Options:
#+OPTIONS: ^:nil ':nil *:t -:t ::t <:t H:3 \n:nil arch:headline
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: f:t inline:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t todo:t |:t

#+OPTIONS: author:Johannes Wasmer
#+OPTIONS: email:johannes.wasmer@gmail.com
# #+AUTHOR: Johannes Wasmer
# #+EMAIL: johannes.wasmer@gmail.com

# for org for web (eg gitlab, github): num:nil, toc:nil. using custom Table of Contents below.
# for tex/pdf export, temporarily: num:t, toc:t. replace * Table of Contents -> * COMMENT Table of Contents.
#+OPTIONS: num:nil
# t or nil: disable export latex section numbering for org headings
#+OPTIONS: toc:nil
# t or nil: no table of contents (doesn't work if num:nil)

#+TITLE: dtc-workshop-notes-wasmer
#+SUBTITLE:
#+DATE: <2023-05-14 Sun>
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 28.2 (Org mode 9.6.5)

# ---------------------
# LaTeX Export Options:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[top=0.5in,bottom=0.5in,left=1in,right=1in,includeheadfoot]{geometry} % wider page; load BEFORE fancyhdr
#+LATEX_HEADER: \usepackage[inline]{enumitem} % for customization of itemize, enumerate envs
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: % override 'too deeply nested error'
#+LATEX_HEADER: % (may occur in deeply nested org files)
#+LATEX_HEADER: % reference: https://stackoverflow.com/a/13120787
#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \setlistdepth{9}
#+LATEX_HEADER: \setlist[itemize,1]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,2]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,3]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,4]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,5]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,6]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,7]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,8]{label=$\bullet$}
#+LATEX_HEADER: \setlist[itemize,9]{label=$\bullet$}
#+LATEX_HEADER: \renewlist{itemize}{itemize}{9}
#+LATEX_HEADER:
#+LATEX_HEADER_EXTRA:
#+LATEX_COMPILER: pdflatex

# auto-id: get export-safe org-mode headline IDs
# References:
# - web: https://writequit.org/articles/emacs-org-mode-generate-ids.html
# - local:
#   - Emacs Config Notes > get export-safe org-mode headline IDs
#   - emacs dotfile > =JW 220419 org-mode headlines CUSTOM_ID=
#+OPTIONS: auto-id:t

# --------------------
# Agenda Config.
# Notes:
# - tags:
#   - :TOC: automatic table of contents generation via https://github.com/snosov1/toc-org.
#     (Note: this is for org/markdown etc. For latex/html export, prefer #+OPTIONS: toc:t.)
#+TODO: DOING(1) NEXT(2) TODO(3) WAITING(4) POSTPONED(5) SHELVED(6) | DONE(0) ABANDONED(9)
#+TAGS: URGENT(0) PRIO1(1) PRIO2(2) PRIO3(3) ADMIN(a) CODING(c) WRITING(w) TOC(t)
#+ARCHIVE: dtc-workshop-notes-wasmer_archive.org::

* Description
:PROPERTIES:
:CUSTOM_ID: h-3B113F99-7977-4290-B11D-3817FEF8074B
:END:

Log of my workthrough of the [[https://iterative.ai/blog/jupyter-notebook-dvc-pipeline/][DVC tutorial]] "GitOps for ML: Converting Notebooks
to Reproducible Pipelines".

Throughout, I add notes on application of this tutorial to two of my current
applicable projects. I label these notes with headings "Application to personal
projects".

- [[https://iffgit.fz-juelich.de/phd-project-wasmer/projects/single-impurity-database][single-impurity-database]] > notebooks [[https://iffgit.fz-juelich.de/phd-project-wasmer/projects/single-impurity-database/-/tree/master/notebooks/data_generation][data generation]].
- [[https://iffgit.fz-juelich.de/phd-project-wasmer/projects/jij-prediction][jij-prediction]] > notebook "model training"
  - at time of writing, this notebook is stored here [[https://iffgit.fz-juelich.de/phd-project-wasmer/teaching/sisclab2022-project6-git/-/blob/skm23/notebooks/work-package-2/johannes/skm23/skm23c-model-training.ipynb][model training]], but will be
    moved to jij-prediction later.
* Table of Contents                                                     :TOC_2_gh:noexport:
:PROPERTIES:
:CUSTOM_ID: h-A85C6821-B59A-4F6E-B710-D106E4ED2218
:END:
- [[#description][Description]]
- [[#useful-links][Useful links]]
- [[#intro-and-motivating-example][Intro and motivating example]]
- [[#done-fork-the-tutorial-repo][DONE Fork the tutorial repo]]
- [[#done-download-the-data][DONE Download the data]]
- [[#done-install-python-environment][DONE Install Python environment]]
  - [[#done-install-python-environment---part-1][DONE Install Python environment - Part 1]]
  - [[#done-install-python-environment---part-2][DONE Install Python environment - Part 2]]
  - [[#install-python-environment---part-3][Install Python environment - Part 3]]
- [[#done-run-the-notebook][DONE Run the notebook]]
  - [[#notebook-intro][Notebook intro]]
  - [[#data-preprocessing][Data preprocessing]]
  - [[#load-training-data-and-create-split][Load training data and create split]]
  - [[#define-model-and-train][Define model and train]]
  - [[#plot-training-history-and-save-model][Plot training history and save model]]
  - [[#validation][Validation]]
- [[#done-setting-up-dvc-and-tracking-data][DONE Setting up DVC and tracking data]]
  - [[#setting-up-dvc-and-tracking-data---dvc-init][Setting up DVC and tracking data - dvc init]]
  - [[#setting-up-dvc-and-tracking-data---git-commit][Setting up DVC and tracking data - git commit]]
  - [[#setting-up-dvc-and-tracking-data---add-a-remote-for-data][Setting up DVC and tracking data - Add a remote for data]]
- [[#done-create-paramsyaml][DONE Create ~params.yaml~]]
- [[#doing-turn-notebook-into-to-python-modules][DOING Turn notebook into to Python modules]]
  - [[#turn-notebook-into-to-python-modules---copy-solution][Turn notebook into to Python modules - Copy solution]]
  - [[#turn-notebook-into-to-python-modules---best-practice][Turn notebook into to Python modules - Best practice]]
  - [[#turn-notebook-into-to-python-modules---run-pipeline][Turn notebook into to Python modules - Run pipeline]]
  - [[#intermezzo---enable-dvc-shell-tab-completion][Intermezzo - Enable DVC shell tab completion]]
  - [[#turn-notebook-into-to-python-modules---debugging][Turn notebook into to Python modules - Debugging]]

* Useful links
:PROPERTIES:
:CUSTOM_ID: h-D60E0825-57F2-48F5-8BA0-A855AE37A1B9
:END:

For the tutorial.

- [[https://iterative.ai/blog/jupyter-notebook-dvc-pipeline/][Tutorial blog article]], 2022. Note that this differs somewhat from the later
  YouTube tutorial below. This workthrough follows the latter.
- [[https://github.com/RCdeWit/dtc-workshop][Tutorial GitHub repository]]
- [[https://www.youtube.com/watch?v=6x6GwtNeYdI][Tutorial YouTube video recording]]. Title "GitOps for ML - How to convert
  machine learning notebooks to reproducible DVC pipelines", date
  <2023-04-04 Tue>.
- [[https://dvc.org/doc][DVC docs]]
- [[https://marketplace.visualstudio.com/items?itemName=Iterative.dvc#what-you-get][DVC VSCode extension]]. "The first experiment tracking interface for an IDE".
  - [[https://github.com/iterative/vscode-dvc#what-you-get][What you get]]
  - [[https://github.com/iterative/vscode-dvc#useful-commands][Useful commands]]
- DVC Cheat sheets
  - [[https://www.globalsqa.com/dvc-cheat-sheet/][DVC Cheat sheet by GlobalSQA]]
  - [[https://dvc.org/doc/command-reference][dvc.org/doc/command-reference]]
  - [[https://derekchia.com/dvc/][derekchia.com - DVC Cheatsheet]]

*<2023-06-06 Tue> Update*. TODO.

- [[https://www.youtube.com/watch?v=3-DG4WS5Ikk][Tutorial YouTube video recording]]. Title "Becoming a Pok√©mon Master
  with DVC: Reproducible Machine Learning Experiments", date <2023-06-06 Tue>.

Beyond the tutorial.

- More resources by Iterative AI.
  - [[https://github.com/iterative/awesome-iterative-projects][Awesome Iterative Projects]]. GitHub awesome list.
  - [[https://www.youtube.com/watch?v=7ZgBydEPHwA][Tutorial YouTube video recording]]. Title "ZnTrack: DVC Machine Learning
    Pipelines in Python and Jupyter Notebooks", date <2023-08-20 Sun>.
    - ZnTrack is a tool built on top of DVC to make DVC easier to use.
    - [[https://github.com/PythonFZ/DVCExample/tree/mnist_sign_language][example repo]], [[https://onedrive.live.com/view.aspx?resid=7FED1240F77A0670!2402&ithint=file%2cpptx&authkey=!ALoB14yh4e7i80w][slides]], [[https://notebooks.gesis.org/binder/jupyter/user/pythonfz-dvcexample-vjjh1ft8/doc/tree/Workflow.ipynb][Binder notebook]], [[https://github.com/zincware/ZnTrack][ZnTrack repo]].
  - [[https://iterative.ai/blog/exp-tracking-dvc-python/?tab=General-Python-API][Instant Experiment Tracking: Just Add DVC!]]
    - Quick-start guide on migrating an existing project, using [[https://dvc.org/doc/dvclive][DVCLive]].

- DagsHub tutorials. DagsHub calls itself the GitHub for data. The platform
  integrates DVC, MLflow, etc. in one platform and wants to make their usage
  easier.
  - Mlflow follow-up tutorials.
    - [[https://www.youtube.com/watch?v=JmCfkpGOE8c][YouTube - DagsHub - MLflow Experiment Tracking Webinar]]. Date <2023-06-13 Tue>.
    - [[https://www.youtube.com/watch?v=K2i-9Gn4XNY][YouTube - DagsHub - MLflow Model Registry Webinar]]. Date <2023-04-06 Thu>.
  - [[https://dagshub.com/khuyentran1401/Machine-learning-pipeline][khuyentran1401/machine-learning-pipeline]]. Tutorial combining MLFlow, Hydra and DVC.

- Other resources.
  - [[https://iffmd.fz-juelich.de/cBVSp52ySMaMnZ4NkAq-MA?view][Workshop Get Started With MLOps]], presented at HDS-LEE Retreat 2023. See DVC.
* Intro and motivating example
:PROPERTIES:
:CUSTOM_ID: h-3EB0EF40-01EA-41B4-851A-7A0FBB4A90CB
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=2m13s][02:13]].

The motivating example is a binary Pokemon CNN classifier. Based on a set of
attributes, the CNN predicts whether a Pokemon is of "water" type or not.

We start with a notebook ~pokemon_classifier.ipynb~ which contains the whole
training pipeline for manual execution, a working model prototype.

Problem: New Pokemon games were released recently. Consequences

- Changed dataset
- Probably model drift
- Changed model performance
- May need retraining

The prototype needs to be changed -> ML experimentation. End up with a lot of
different experiments / models with different parameters. How to keep track,
maintain reproducibility?

We define an experiment = data + code + parameters.

Version control:

- Code, parameters: Git. Basically, all text files.
- Data, models: DVC. Basically, all binary files.

DVC ties your data versioning to your Git commit history. DVC has three main features.

- Data version control
- Pipelines
- Experiments

*DVC data version control.*

Instead of committing the data to Git, DVC commits the data's metadata
~dataset.dvc~ (hash, size, nfiles, ...). This ~.dvc~ file points to something in
the ~.dvc/cache~. DVC can resolve the specific data files in the remote storage
(by default, the local computer; cloud storages get duplicated locally, or sth)
via reflinks. If a new commit changes the ~dataset.dvc~, it can differentially
point to sth else in the cache. For instance, some images in the training data
folder were rmoved, and some added. This avoids data duplication over
incremental changes.

*DVC pipelines.* Timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=8m45s][08:45]].

DVC pipelines are directed acyclic graphs (DAGs) of connected steps or stages.
For instance, data preprocessing, loading, model training, performance
evaluation. Each stage has inputs and outputs. This makes it possible to control
stage execution via DVC. For instance only start data loading once the dataset
labels and images from preprocessing are stored in DVC cache. This makes
pipelines reliable and reproducible. DVC pipelines are described as YAML files
~dvc.yaml~.

(TODO: reproduce the flowcharts shown in video tutorial with mermaid here.)

*DVC experiments.*

DVC pipelines enable experiments. A ~dvc.yaml~ pipeline has inputs code, data,
parameters, and outputs model, plots, metrics. Version control:

- Git: Code, parameters, pipeline, metrics.
- DVC: Data, model, plots.

*A set of specific pipeline, inputs and outputs constitute one experiment = one
Git commit.* Via version control, we can return to any experiment and reproduce
it if needed.

The remainder of this workshop is about transforming the motivating example
Jupyter notebook into such a Git+DVC pipeline.

* DONE Fork the tutorial repo
CLOSED: [2023-05-12 Fri 19:26]
:PROPERTIES:
:CUSTOM_ID: h-201EAD2C-B987-4802-96C1-8C6C319257C1
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=12m8s][12:08]].

I created [[https://github.com/Irratzo/dtc-workshop][a fork]] of the repository and work on that.

I want to evaluate the [[https://marketplace.visualstudio.com/items?itemName=Iterative.dvc][VSCode DVC extension]]. So I do the tutorial two times
simultaneously, once in VSCode with the DVC extension, and once in PyCharm
without. I separate those into the two fork repo branches [[https://github.com/Irratzo/dtc-workshop/tree/vscode-dvc][~vscode-dvc~]] and
[[https://github.com/Irratzo/dtc-workshop/tree/pycharm][~pycharm~]]. These notes are for now only in the ~main~ branch under =notes/=.

Local file repo locations:

- [[file:~/src/github.com/irratzo/forks/dtc-workshop-vscode-dvc][dtc-workshop-vscode-dvc]]. For work on branch ~vscode-dvc~.
- [[file:~/src/github.com/irratzo/forks/dtc-workshop-pycharm/][dtc-workshop-pycharm]]. For work on branch ~pycharm~.
- [[file:~/src/github.com/irratzo/forks/dtc-workshop-solution/][dtc-workshop-solution]]. Solution.

* DONE Download the data
CLOSED: [2023-05-12 Fri 19:26]
:PROPERTIES:
:CUSTOM_ID: h-633EB921-DCE8-4544-84B7-DBFD868CFC60
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=12m8s][12:08]].

Downloaded the training data to repo folder, =./data/external=, unpacked the two
zips into =pokemon/= and =pokemon-images/=.
* DONE Install Python environment
CLOSED: [2023-06-17 Sat 13:46]
:PROPERTIES:
:CUSTOM_ID: h-4622B104-D4EF-4984-B9EC-9BBF56B84F25
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=15m50s][15:50]].

** DONE Install Python environment - Part 1
CLOSED: [2023-05-16 Tue 15:01]
:PROPERTIES:
:CUSTOM_ID: h-ADF231F6-6755-486C-82B4-C53B969BAF5D
:END:

This part: Creating a ~requirements.txt~ file that works for my hardware, Apple
MacBook Pro M2.

For both branches: the [[https://github.com/RCdeWit/dtc-workshop/blob/e69b85bd79602d6491b52da32569e4e6331373a9/requirements.txt#L1][requirements.txt]]

- assumes strict version constraints for compatibility
- assumes as hardware an older Apple Mac with M1 chip. That's why they use
  =tensorflow-macos= and =tensorflow-metal=, and specific versions.For other
  hardware, such as my M2 chip,replace with =tensorflow=. In the video, they
  replace with ~tensorflow==2.11.0=~

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop/requirements.txt
#+end_src

#+RESULTS:
#+begin_example
notebook==6.5.2
dvc[all]==2.44.0
tensorflow-macos==2.9
tensorflow-metal==0.5.0
pandas==1.5.3
pillow==9.4.0
matplotlib==3.6.3
scikit-learn==1.2.1
isort==5.12.0
pickle-mixin==1.0.2
#+end_example


I deviate from that.

In both branches, I replace the M1 tensorflow versions with =tensorflow=. I
replace =notebook= with =jupyterlab=, cause it is the sucessor and I prefer to
have it. Update: ADD =jupyterlab= to =notebook= because your IDE (eg PyCharm)
might require =notebook= in the environment. Both are developed in conjunction,
but are completely separate.

In branch ~vscode-dvc~, VSCode extension DVC version v08.11 complained that
extension is not compatible with ~dvc[all]==2.44.0~ and requires at least
~dvc[all]==2.56.0~. So, I lift all veersion constraints here.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop/requirements-original.txt
#+end_src

#+RESULTS:
#+begin_example
notebook==6.5.2
dvc[all]==2.44.0
tensorflow-macos==2.9
tensorflow-metal==0.5.0
pandas==1.5.3
pillow==9.4.0
matplotlib==3.6.3
scikit-learn==1.2.1
isort==5.12.0
pickle-mixin==1.0.2
#+end_example

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop/requirements.txt
#+end_src

#+RESULTS:
: jupyterlab
: dvc[all]
: tensorflow
: pandas
: pillow
: matplotlib
: scikit-learn
: isort
: pickle-mixin

In branch ~pycharm~, I only adopt the same ~tensorflow==2.11.0~ version as in
the tutorial video and leaving everything else as is produced a patchy
environment. So I also went with the constraintless reqs version here. I could
enforce ~dvc[all]==2.56.0~ here since not bound by DVC extension. But better
keep needed adjustments consistent between both branches, so same env. As for
the =pickle-mixin=, I can always commit a freeze env later (pickle serializes
Python objects; deserialization is only guaranteed to work with the exact same
package versions), if DVC does not already support on its own.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/requirements.txt
#+end_src

#+RESULTS:
: jupyterlab
: notebook
: dvc[all]
: tensorflow
: pandas
: pillow
: matplotlib
: scikit-learn
: isort
: pickle-mixin

(Update <2023-05-16 Tue>: Added ~notebook~ cause PyCharm Jupyter notebooks
require ~notebook~ not ~jupyterlab~ to work properly, see my [[https://youtrack.jetbrains.com/issue/PY-35688/Jupyter-notebook-using-wrong-executable-and-path#focus=Comments-27-7335157.0-0][error & solution
report]].)

In both branches, I rename the old requirements file to
=requirements-original.txt= and the new one to =requirements.txt=. Both IDEs by
default install env fixed on this filename, so this swap makes that easier.

*How to create a Python environment from requirements.txt with IDEs VSCode, PyCharm*.

Create venv/pip env from requirements file in PyCharm. Note that PyCharm
automatically selects the file ~requirements.txt~ for this. Project Settings >
Add interpreter > PyCharm creates the env in the repo folder =./venv=. Create,
done.

Create venv/pip env from requirements file in VSCode. Command Palette >
Python: Create environment > Leave all default (package manager venv, Python
version, requirements file selection). Create. VSCode creates the env in the
repo folder =./.venv=.

Side note: To delete the env, eg if something went wrong, in both cases, just
remove the corresponding folder and repeat process.

Now I freeze the installed environments.

In PyCharm, Tools > Sync Python Environments did not work for me.

So, in both branches / IDEs, I did ~pip freeze > requirements.txt~, hand-picked
out above libraries (ie, delete all others from the file), and overwrote
=requirements.txt= with that again.

(While doing it also found out, that again, PyCharm had not installed many of
the reqs in the env, even without version constraints. So, next time do it with
~pip~ direcly, in the first place ... The env install via VSCode worked,
however.)

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop/requirements.txt
#+end_src

#+RESULTS:
: jupyterlab==3.6.3
: dvc[all]==2.56.0
: tensorflow==2.12.0
: pandas==2.0.1
: Pillow==9.5.0
: matplotlib==3.7.1
: scikit-learn==1.2.2
: isort==5.12.0
: pickle-mixin==1.0.2

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/requirements.txt
#+end_src

#+RESULTS:
: jupyterlab==3.6.3
: notebook==6.5.4
: dvc[all]==2.56.0
: tensorflow==2.12.0
: pandas==2.0.1
: Pillow==9.5.0
: matplotlib==3.7.1
: scikit-learn==1.2.2
: isort==5.12.0
: pickle-mixin==1.0.2

However, then I found out that on my MacBook Pro M2, these Tensorflow
installations did not work. So, got to do an intermezzo, how to install
TensorFlow on Apple M2 in 2023-05. Putting that in phd-project-wasmer > work
journal > install tensorflow, pytorch, jax on Apple M2

- [[https://iffgit.fz-juelich.de/phd-project-wasmer/notes/public/-/blob/main/work/work-journal/themed/2023-05-13-deep-learning-on-apple-m2/deep-learning-on-apple-m2.org][web link]]
- [[file:~/src/iffgit.fz-juelich.de/phd-project-wasmer/notes/main/public/work/work-journal/themed/2023-05-13-deep-learning-on-apple-m2/deep-learning-on-apple-m2.org][local file link]]

After analysis there, it turns out that venv/pip env creation with
VSCode/PyCharm venv/pip env creation features does NOT produce a working
TensorFlow installation, but doing it by hand with venv/pip from same
requirements file without version constraints DOES produce a working TensorFlow
installation. So, I did that instead, removed the environments created by the
two respective IDEs, and copied the resulting environment folder into the
respective VSCode / PyCharm projects.

Update: pip/venv environments are tied to their locations. The paths are stored
in the venv's config files. When moving or copy-pasting them, update those
references by hand to the new location. Google how to do that.

#+begin_src shell
rm -rf ~/src/github.com/irratzo/forks/dtc-workshop/.venv
rm -rf ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/venv

cp -r ~/venvs/venv-dtc-workshop ~/src/github.com/irratzo/forks/dtc-workshop/.venv
cp -r ~/venvs/venv-dtc-workshop ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/venv

rm -rf ~/venvs/venv-dtc-workshop ~/venvs/venv-dtc-workshop-requirements.txt
#+end_src

Here is the pinned requirements of that env after install from requirements with
not version constraints, performed on <2023-05-14 Sun>, now the same for both
branches.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop/requirements.txt
#+end_src

#+RESULTS:
: jupyterlab==3.6.3
: dvc[all]==2.56.0
: tensorflow==2.13.0rc0
: pandas==2.0.1
: Pillow==9.5.0
: matplotlib==3.7.1
: scikit-learn==1.2.2
: isort==5.12.0
: pickle-mixin==1.0.2

Select the new env.

In VSCode, Command Palette > Python: Select interpreter.

In PyCharm, Project Settings > Pyton Interpreter.

Finally, check that the env now works, including TensorFlow.

In both editors, open the classification Jupyter notebook, and run the "Imports"
cell. It should run now without error. Maybe have to select the correct kernel
first.

** DONE Install Python environment - Part 2
CLOSED: [2023-06-17 Sat 13:45]
:PROPERTIES:
:CUSTOM_ID: h-A1BB891F-FBB9-4F63-BE51-7D22BB0D74DC
:END:

This part: Recreating by hand and updating both environments.

Update <2023-06-17 Sat>. Turns out that, for whatever reason the envs had worked
initially, they did not work anymore after returning to this project after some
while. Turns out that

- 1) pip/venv envs are hardcoded to their creation location (in
  =venv/bin/activate*= scripts, variable ~VIRTUAL_ENV~). So moving them around,
  like I did in part 1, will make them unusable, initially. This can be easily
  fixed, however, by replacing the ~VIRTUAL_ENV~ value with the new location.
- 2) The =venv/bin/python= was missing. When activated (after fixing 1)), they
  used the system Python =/usr/local/bin/python=, instead. I believe now that
  this is an issue with my conda / mamba / micromamba ~base~ environment.
  Details see [[file:~/Desktop/Coding/Python/PythonConfig-Mac_Notes.org::*2023-06-17 base environment may be damaged][here]] (local file link; see section "2023-06-17 micromamba base
  environment may be damaged").

So, removed both envs again. Recreate only one at current local project
location. Use this one env for both IDEs / project branches. Note that currently
on MY system, I MUST write ~python3~ instead of ~python~ when creating the env
cause latter is Python 2 and creation would fail with it. In any case, just
check version before.

To repeat: From now on, *there is only one env*, not two.

#+begin_src shell
# only if needed: update requirements.txt dvc version to the one
# currently required by VSCode DVC extension. As of 2023-06-18:
# dvc[all]>=2.58.0

# go to project
cd ~/src/github.com/irratzo/forks/dtc-workshop-pycharm
# remove old env, if one exists
rm -rf venv

# create new env. use Python version currently reccomended by
# https://www.tensorflow.org/install/pip: Python 3.9
# here, coming from a separate conda env
~/micromamba/envs/py39env/bin/python -m venv venv
source venv/bin/activate

# install tensorflow
pip install --upgrade pip
pip install tensorflow
# verify installation
python3 -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
# install env requirements
pip install -r requirements.txt

# # alternative: install tensorflow and requirements all in one go; should make no difference
# pip install --upgrade pip
# pip install -r requirements.txt
#+end_src

That worked. Done.

In case installing / using TensorFlow pip/venv env fails again, see
deep-learning-on-apple-m2.org notes for current working method.

- [[https://iffgit.fz-juelich.de/phd-project-wasmer/notes/public/-/blob/main/work/work-journal/themed/2023-05-13-deep-learning-on-apple-m2/deep-learning-on-apple-m2.org][web link]]
- [[file:~/src/iffgit.fz-juelich.de/phd-project-wasmer/notes/main/public/work/work-journal/themed/2023-05-13-deep-learning-on-apple-m2/deep-learning-on-apple-m2.org][local file link]]

Updated frozen requirements.txt.

#+begin_example
jupyterlab
notebook
dvc[all]>=2.58.0
tensorflow
pandas
Pillow
matplotlib
scikit-learn
isort
pickle-mixin
#+end_example
** Install Python environment - Part 3
:PROPERTIES:
:CUSTOM_ID: h-BC3BBDC3-5D84-439A-BE71-2CFCD5ACD5AE
:END:
Date <2023-11-14 Tue>.

Returned to this project on <2023-11-14 Tue> after a second long hiatus after
last work on it since <2023-06-18 Sun>. Got to check now that the Python
environment is still there, and that they still work. Then, check on the IDEs,
VSCode and PyCharm, and whether the envs work with them. This includes checking
compatibility with the version of the VSCode DVC extension.
* DONE Run the notebook
CLOSED: [2023-05-16 Tue 22:54]
:PROPERTIES:
:CUSTOM_ID: h-344A9323-DF4C-4126-919A-B0D85FBAF134
:END:
** Notebook intro
:PROPERTIES:
:CUSTOM_ID: h-EB2D69FE-671E-4DCF-8D0F-08A91D08F9C2
:END:
Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=21m38s][21:38]].

Run the notebook ~pokemon_classifier.ipynb~ and explain what it does.

I added minor additional explanations right inside the notebook , for now on
branch ~pycharm~.

Explanation of the Jupyter notebook model pipeline, Pokemon binary classifier,
CNN model, TensorFlow.

The dataset before preprocessing consists of a CSV table with 802 samples, and a
folder of images, one Pokemon per image.

#+begin_src shell :results output
ls ~/src/github.com/irratzo/forks/dtc-workshop/data/external/images | wc -l
#+end_src

#+RESULTS:
:      905

** Data preprocessing
:PROPERTIES:
:CUSTOM_ID: h-28B68A92-46D7-40B2-AD83-A9368C781680
:END:

"Not really important to understand the content of these cells before we replace
them later on."

Functions / cells (code snippets) in this section, in order.

| Function                     |
|------------------------------|
| ~preprocess_training_labels~ |
| ~load_training_data~         |

The function ~preprocess_training_labels~ adds a one-hot encording to the
original table for each of the two types that a Pokemon posesses. Eg for row 1,
Pokemon No. 1, type1=grass, type2=poison turns into isFire=0, isGrass=1, ...,
isGround=0.

The function ~preprocess_training_data~ add the image filepaths to the table and
moves the image files to the =data/processed= directory.

** Load training data and create split
:PROPERTIES:
:CUSTOM_ID: h-568A779F-3013-483B-A29F-5D8710F04291
:END:

Functions / cells (code snippets) in this section, in order.

| Function                        |
|---------------------------------|
| ~load_training_data~            |
| ~create_labels~                 |
| ~train_test_split~              |
| Cell "Save train and test data" |

The function ~load_training_data~ uses ~tf.keras.utils~ functions ~load_img~ and
~img_to_array~ to load images from table, convert to list of Numpy arrays, and
return as one large array ~X~.

Explanation for the shape of the training data ~X~. The first dimension is the
number of images. The second and third dimensions are the height and width of
the image. The fourth dimension is the number of channels. The images are RGBA,
so there are four channels.

(Thanks, GitHub Copilot comment completion.)

The function ~create_labels~ just returns a DataFrame with one column, is a
Pokemon a Water Pokemon, yes/no (one-hot).

The "Train test split" cell creates a 0.8/0.2 train-test split of (X,y). Notice
that the ~SEED~ for the random split was defined as a global constant in the
beginning, to get the same train-test split upon rerun.

The function "Save train and test data" saves all data objects ~X, X_train,
X_test, y, y_train, y_test~ as respective file dumps ~X.pckl~, etc., using
function ~pickle.dump~. This most simple method of serialization is not usually
done in production, but serves as an intermediate step towards a DVC pipeline, a
DAG, where each stage has (data) inputs and outpus, see intro. Also note, that
~pickle~ as serialization solution here is just for purpose of an easy demo, not
something used in production.
** Define model and train
:PROPERTIES:
:CUSTOM_ID: h-5F64A753-138E-464E-89CC-A67DF8290603
:END:

The function ~compile_model~ uses [[https://keras.io/api/models/sequential/][Keras Sequential]] class to define a
convolutional neural network (CNN) for the given image dimensions in the
dataset. Note that all model architecture hyperparameters are hardcoded inside
the function.

As I am doing this on Apple M2, note also this line inside the function.

#+begin_src python
# Legacy needed for M1/M2
optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001) #Adam, RMSprop or SGD
#+end_src

The resulting model is fairly small, with only ~1k parameters. This is on the
same order as the number of data samples.

The function ~train_estimator~ trains the model, calling ~model.fit~.

As I am personally still in the "traditional ML" mode of understanding, vs. the
deep learning way of doings things in this tutorial, here is a clarification on
terminology wrt train, validate, test data. Note ~validation_data=(X_test,
y_test)~. So, there is no final "test data" on which the model performance is
evaluated, as is done in traditional ML after cross-validation. I guess, this
CAN be done in deep learning as well, but not really needed. Instead, the model
is iteratively evaluated on this validation data.

~MODEL_EPOCHS~ and ~MODEL_BATCH_SIZE~ are also global constants defined in the
beginning.

The inner function calculating the ~class_weight~ (for classification tasks)
remedies the class imbalance wrt Water Pokemons overrepresentation. From Keras
Model docs:

#+begin_quote
~class_weight~: Optional dictionary mapping class indices (integers) to a weight
(float) value, used for weighting the loss function (during training only). This
can be useful to tell the model to "pay more attention" to samples from an
under-represented class.
#+end_quote

** Plot training history and save model
:PROPERTIES:
:CUSTOM_ID: h-2696EE8D-5E77-46CF-88EB-EA99C46791F9
:END:

The function ~save_estimator~ plots loss and accuracy of the model during
training, and saves the model to disk under ~$PROJECT/outputs/model~. With
Keras, we don't need to record those separately, but can access it after
training from ~estimator.history~.

Note that the [[https://keras.io/api/saving/][Keras save]] method saves the model as a /directory/, not a single
file. The ~.pb~ file format is Google's [[https://github.com/protocolbuffers/protobuf][protobuf]] format. It can store, among
other things, TensorFlow neural network.

#+begin_src shell :results output
tree ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/outputs/
#+end_src

#+RESULTS:
#+begin_example
~/src/github.com/irratzo/forks/dtc-workshop-pycharm/outputs/
|-- model
|   |-- assets
|   |-- fingerprint.pb
|   |-- keras_metadata.pb
|   |-- saved_model.pb
|   `-- variables
|       |-- variables.data-00000-of-00001
|       `-- variables.index
`-- train_history.png

3 directories, 6 files
#+end_example

** Validation
:PROPERTIES:
:CUSTOM_ID: h-35A75BA7-5A0B-416B-9724-1C5F20498992
:END:

With "validation" here, model performance, model evaluation is meant, NOT data
split train/test.)

The function ~predict_pokemon~ takes the trained model for a spin, predicting
whether a given Pokemon is Water type or not, and printing its image.

The next cell loads the model and data from disk, as part of the pipeline
DAG I/O perspective. From the DVC pipeline DAG I/O perspective, the validation
stage requires to load the model and the data from disk, as input.

Next, the ~predictions = model.predict(X) > 0.5~ is called (returns a True/False
binary classification Numpy array for each sample). These are then used to
compute model metrics for classification tasks, accuracy, precision, recall and
F1 score. Note that these are computed on the /whole/ dataset from the predicted
and true labels ~(predictions, y)~.

Finally, a confusion matrix is plotted and saved. Note that the model almost
always classifies Water Pokemons correctly, but is not better at classifying
non-Water Pokemon than random guessing. Again, probably due to class imbalance.

The tutor again emphasized, such a model would not be put into production, it is
merely a working example for this tutorial.

So, this is the model prototype.

At the end of the notebook run, the ~data~ folder looks like this.

#+begin_src shell :results output
cd ~/src/github.com/irratzo/forks/dtc-workshop-pycharm
tree -L 2 data/
#+end_src

#+RESULTS:
#+begin_example
data/
|-- external
|   |-- images
|   |-- pokemon-images.zip
|   |-- pokemon.zip
|   `-- stats
|-- external.dvc
`-- processed
    |-- X.pckl
    |-- X_test.pckl
    |-- X_train.pckl
    |-- pokemon
    |-- pokemon-with-image-paths.csv
    |-- pokemon.csv
    |-- y.pckl
    |-- y_test.pckl
    `-- y_train.pckl

5 directories, 11 files
#+end_example

* DONE Setting up DVC and tracking data
CLOSED: [2023-05-17 Wed 10:21]
:PROPERTIES:
:CUSTOM_ID: h-5647513C-C42B-45A2-A974-90F10E24BD6C
:END:
** Setting up DVC and tracking data - dvc init
:PROPERTIES:
:CUSTOM_ID: h-D43969B6-78FF-45E5-8DB2-636BBDFCC1C6
:END:

Tutorial repo [[https://github.com/RCdeWit/dtc-workshop#setting-up-dvc-and-tracking-data][section]].

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=30m25s][30:25]].

#+begin_quote
This point may be familiar to you: a working prototype in a notebook. Now, how
do we transform it into a reproducible DVC pipeline?
#+end_quote

From the motivating example, say we change the dataset because of integration of
a new set of Pokemons. Then we run another experiment characterized by this
changed data, and maybe we also want to adapt the model or try out different
parameters. Then, all of the serialized input and output data and models and
metrics would be overwritten. We don't want that. So we'll start data versioning
with DVC now.

Start with adding the first input data.

Input in branches ~pycharm~ PyCharm terminal / ~vscode-dvc~ VSCode terminal.

Note that ~dvc~ CLI is modeled closely on the ~git~ CLI, so often, a Git+DVC
project vs. a Git DVC project requires to repeat the same or similar command
twice, once for code and once for data.

#+begin_src shell
# init DVC
dvc init
# if .dvc/ already exists
dvc init -f

# add external data
dvc add data/external/
#+end_src

This has two effects. The directory is added to the local ~.gitignore~, and a
file ~external.dvc~ is created.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/data/.gitignore
#+end_src

#+RESULTS:
: /external
: outs:
: - md5: 8caf358d685344d3eb8b0ee6783275ff.dir
:   size: 235910211
:   nfiles: 908
:   path: external

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/data/external.dvc
#+end_src

#+RESULTS:
: outs:
: - md5: 8caf358d685344d3eb8b0ee6783275ff.dir
:   size: 235910211
:   nfiles: 908
:   path: external

Only evident in the tutorial video: ~git status~ reveals that also created three
new files were created, ~.dvc/.gitignore~, ~.dvc/config, ~.dvcignore~. This is
not evident when starting with the tutorial repo, cause it already has a
~.dvc/~, so DVC was already initialized.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/.dvc/.gitignore
#+end_src

#+RESULTS:
: /config.local
: /tmp
: /cache

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/.dvc/config
#+end_src

#+RESULTS:

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/.dvcignore
#+end_src

#+RESULTS:
: # Add patterns of files dvc should ignore, which could improve
: # the performance. Learn more at
: # https://dvc.org/doc/user-guide/dvcignore

*Application to personal projects.*

- single-impurity-database.
  - Add script / notebook cell to download [[https://molmod.ugent.be/deltacodesdft][deltacodesdft]] structures
  - Add as ~data/external~
- jij-prediction.
  - add AiiDA-exported data as ~data/external~ or ~data/input~.
** Setting up DVC and tracking data - git commit
:PROPERTIES:
:CUSTOM_ID: h-AFEC4985-9CB5-4DED-8419-6E061FAB3C76
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=33m22s][33:22]].

The video tutorial now does this.

#+begin_src shell
git add .dvc; git commit -m "dvc init, dvc add /data/external"
#+end_src

This is why the tutorial repo branch ~main~ already has a ~.dvc~ folder. The
tutor says that he should have done that in a branch ~practice~, really, and
switches to that branch for the remainder of the tutorial.

To reinit the repo to the state before DVC, I apply this manual fix in both
my tutorial branches ~pycharm~ and ~vscode-dvc~.

#+begin_src shell
# reinit repo to state before DVC
rm -rf data/.gitignore data/external.dvc .dvcignore .dvc
git add .dvcignore .dvc/ ; git commit -m "undo tutorial dvc init ..." -m "- tutorial by accident already did 'dvc init' and 'dvc add data/external' on branch main" -m "- tutorial then switched to branch practice, but left original commit in main" -m "- this commit resets the repo to the state before DVC init, by removing all DVC changes"

# add notebook change after 'Run the notebook'
git add notebooks/ ; git commit -m "notebook state after tutorial section 'run the notebook'"
git push origin BRANCH
#+end_src

Now, can do the DVC init step again, as if it were the first time. We'll add the
associated metadata files to Git in the same go.

#+begin_src shell
# init DVC
dvc init; git add .dvcignore .dvc/; git commit -m "dvc init"

# add external data
dvc add data/external/; git add data/external.dvc data/.gitignore; git commit -m "dvc add /data/external"
#+end_src

** Setting up DVC and tracking data - Add a remote for data
:PROPERTIES:
:CUSTOM_ID: h-BA6D6140-CF9E-47D6-A255-CC97C125EABB
:END:

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=35m0s][35:00]].

Just as Git has a remote storage for code and associate command ~git push~, in
this case GitHub, DVC allows to add a remote for data and associate command ~dvc
push~.

See [[https://dvc.org/doc/user-guide/data-management/remote-storage#remote-storage][DVC docs - Remote Storage]].

Video tutorial uses [[https://dvc.org/doc/user-guide/data-management/remote-storage/google-drive#how-to-setup-a-google-drive-dvc-remote][Google Drive]] as remote storage for demo puposes. I, however,
use self-hosted SSH.

- DVC allows [[https://dvc.org/doc/user-guide/data-management/remote-storage#self-hosted--on-premises][self-hosted remote storage]]: SSH, HDFS, HTTP, WebDAV.
- Try to set this up for my projects to sync to ~/Data/division/iff-user~ via
  SSH. (Replace ~iff-user~ placeholder with real username.)

Okay, on SSH remote ~ifflinux~, have now set up remote directory
=/Data/division/iff-user/dvc/dtc-workshop= for this project. Try adding that now
following the [[https://dvc.org/doc/user-guide/data-management/remote-storage/ssh][DVC docs - SSH]] page.

As docs suggest, first verify connection works for ~ssh~ and ~sftp~.

#+begin_src shell
ssh iff-user@ifflinux.iff.kfa-juelich.de
logout
sftp iff-user@ifflinux.iff.kfa-juelich.de
exit
#+end_src

Now, on branch ~pycharm~. Add the remote and push the DVC commits.

#+begin_src shell
# # add remote to repo
# dvc remote add -d myremote ssh://user@example.com:2222/path
dvc remote add -d iff-main ssh://iff-user@ifflinux.iff.kfa-juelich.de/Data/division/iff-user/dvc/dtc-workshop

# Actually, later I changed the remote storage location again, to reflect the
# folder structure in my ~/src/ folder, which in turn reflects all my projects'
# source code remote locations. Now, the folder trees of ~/src and ~/dvc/ can
# be identical, avoiding confusion.
dvc remote modify iff-main url ssh://wasmer@ifflinux.iff.kfa-juelich.de/Data/ias-1/wasmer/dvc/github.com/irratzo/forks/dtc-workshop

# # add SSH key for SSH remote login
# dvc remote modify --local myremote keyfile /path/to/keyfile
dvc remote modify --local iff-main keyfile ~/.ssh/id_rsa

# # push commits (here, data/external) to remote, as initial test
dvc push

# # if successful, commit config change
git add .dvc ; git commit -m "dvc remote add -d iff-main (remote repo for project data)"
git push origin pycharm
#+end_src

The ~dvc remote add~ updated the DVC config.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/.dvc/config
#+end_src

#+RESULTS:
: [core]
:     remote = iff-main
: ['remote "iff-main"']
:     url = ssh://wasmer@ifflinux.iff.kfa-juelich.de/Data/ias-1/wasmer/dvc/github.com/irratzo/forks/dtc-workshop

After the ~dvc push~, the remote storage is populated. But not with the
identical content of ~data/external~, but instead hash-based folders ~00/~ to
~ff/~, with hash-based files in them, like ~ff/e39770f78253994bf4a4db7e8d6d7b~.
Looks a lot like the old AiiDA remote (working directory) file storage scheme
(which they replaced with object storage cause this method produced too many
inodes ... doesn't DVC potentially cause the same problem here?). These
correspond to the file hashes stored in the repo's ~.dvc/cache/~.

Now, the interesting question is, how to sync that with the branch ~vscode-dvc~?
I can't do the same and ~dvc push~ there, since the data is already in the
remote. So instead, I could try to tmp move local ~data/external~ to the side,
do a ~dvc pull~ instead and see if the data gets downloaded.

On branch ~vscode-dvc~.

#+begin_src shell
# (all except third command same as above in other repo)

# # add remote to repo
# dvc remote add -d myremote ssh://user@example.com:2222/path
dvc remote add -d iff-main ssh://iff-user@ifflinux.iff.kfa-juelich.de/Data/division/iff-user/dvc/dtc-workshop

# # add SSH key for SSH remote login
# dvc remote modify --local myremote keyfile /path/to/keyfile
dvc remote modify --local iff-main keyfile ~/.ssh/id_rsa

# remove (or mv backup) local data/external and replace with the one from remote
rm -rf data/external
dvc pull

# if successful, commit config change
git add .dvc ; git commit -m "dvc remote add -d iff-main (remote repo for project data)"
git push origin vscode-dvc
#+end_src

It worked! ~dvc pull~ restored the original ~data/external~ contents from the
remote. Note here that this also worked because the DVC remote does not have any
branches, or we at least have not specified them. So, differently from the code
remote repo, all code branches see the same remote DVC data.

*Application to personal projects.*

- Set this up for all my personal projects with data. Also use
  ~/Data/division/username/dvc/~ as base. Mind the ~quota~ on ~/Data~.
  - single-impurity-database.
    - all data not stored in AiiDA / iffAiiDA.
    - Minor detail: What about archives exported from AiiDA DB?
  - jij-prediction.

* DONE Create ~params.yaml~
CLOSED: [2023-05-17 Wed 11:52]
:PROPERTIES:
:CUSTOM_ID: h-FF6779C7-EFC7-48F3-A552-5CCFDE258EA5
:END:

Tutorial repo [[https://github.com/RCdeWit/dtc-workshop#create-paramsyaml][section]].

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=41m37s][41:37]].

Now we are finally set up to start building a [[https://dvc.org/doc/user-guide/pipelines#pipelines][DVC pipeline]].

Create ~./params.yaml~, same content as in tutorial repo README section. Similar
to parameters in notebook.

#+begin_src shell :results output
cat ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/params.yaml
#+end_src

#+RESULTS:
#+begin_example
base:
  seed: 42
  pokemon_type_train: "Water"

data_preprocess:
  source_directory: 'data/external'
  destination_directory: 'data/processed'
  dataset_labels: 'stats/pokemon-gen-1-8.csv'
  dataset_images: 'images'

train:
  test_size: 0.2
  learning_rate: 0.001
  epochs: 15
  batch_size: 120
#+end_example

#+begin_src shell
git add params.yaml; git commit -m "start build DVC pipeline, create params.yaml" ; git push origin branch-name
#+end_src

* DOING Turn notebook into to Python modules
:PROPERTIES:
:CUSTOM_ID: h-A7282650-BDA3-4A1B-94DC-01782A5D9AE3
:END:
In tutorial repo, this section is called "Creat Python modules".
** Turn notebook into to Python modules - Copy solution
:PROPERTIES:
:CUSTOM_ID: h-96663827-0B41-4373-A8A0-5413222DF07D
:END:

Tutorial repo [[https://github.com/RCdeWit/dtc-workshop#create-python-modules][section]].

Tutorial video timestamp [[https://www.youtube.com/watch?v=6x6GwtNeYdI&t=42m50s][42:50]].

Turn the notebook prototype into Python modules. We'll use the solution's
[[https://github.com/RCdeWit/dtc-workshop/commit/1881a0e6cf9379eea53355f6e3fdd76890f48eac][specific commit]], where the tutor committed the finished ~src/~ folder from this
step to the solution repo.

#+begin_src shell
git clone git@github.com:RCdeWit/dtc-workshop.git dtc-workshop-solution
cd dtc-workshop-solution
git checkout 1881a0e6cf9379eea53355f6e3fdd76890f48eac
cp -r src ../dtc-workshop-pycharm
cp -r src ../dtc-workshop-vscode-dvc
#+end_src

#+begin_src shell :results output
tree -I "*.pyc" -I "__pycache__" ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/src
#+end_src

#+RESULTS:
: ~/src/github.com/irratzo/forks/dtc-workshop-pycharm/src
: |-- data_load.py
: |-- data_preprocess.py
: |-- evaluate.py
: |-- train.py
: `-- utils
:     `-- find_project_root.py
:
: 1 directory, 5 files

** Turn notebook into to Python modules - Best practice
:PROPERTIES:
:CUSTOM_ID: h-9349040B-C0ED-4EFA-B754-C774BB529D82
:END:

Some notes on how the tutor turned the notebook cells into modules. These can
serve as *best practices* / *guideline* for manually turning an ML training
notebook and indeed, any kind of computational pipeline, into Python modules.

- Each module represents one pipeline stage now, with file consecutive input and
  output (stage X output is input for stage Y).
- Each module is script-enabled with an ~argparse~ CLI, by having a ~__main__~
  section at the end that loads the ~params.yaml~. Then, global constants needed
  for this stage are read from the ~params.yaml~.

  Example.

  #+begin_src python
  if __name__ == '__main__':

      args_parser = argparse.ArgumentParser()
      args_parser.add_argument('--params', dest='params', required=True)
      args = args_parser.parse_args()

      with open(args.params) as param_file:
          params = yaml.safe_load(param_file)

      PROJECT_ROOT = find_project_root()

      SOURCE_DIRECTORY: str = params['data_preprocess']['source_directory']
  #+end_src

- The only non-hard-coded relative project directory / location is the absolute
  directory of the project root itself. Since this is user-specific, this is
  instead in function ~utils.find_project_root~. It goes up through parent
  folders and stops once it has found one which contains a ~.git/~.
- Some notebook cells get turned into module functions, others are part of the
  module's ~__main__~ section, which also calls the functions.
- In ~data_preprocessing~ main section, preprocessing functions are called on
  the external data.
- In ~data_preprocessing~ main section, the preprocesed data is loaded,
  train/test split performed, and the result saved as pickle files as training
  data.
- In ~train~ main section, the training data is loaded from disk, the model is
  compiled, trained and saved to disk.
- In ~evaluate~ main section, the model and train data is loaded, the evaluation
  metrics are computed, plotted and saved to disk.

** Turn notebook into to Python modules - Run pipeline
:PROPERTIES:
:CUSTOM_ID: h-991888A6-E963-4816-BCDC-0EE7D9868B58
:END:

With the ~src~ package finished, the same pototype pipeline from the notebook
can now be run from the command line instead by running all notebook cells (and
hope that they are still correct for that).

#+begin_src shell
python src/data_preprocess.py --params params.yaml
python src/data_load.py --params params.yaml
python src/train.py --params params.yaml
python src/evaluate.py --params params.yaml
#+end_src

** Intermezzo - Enable DVC shell tab completion
:PROPERTIES:
:CUSTOM_ID: h-2ED0E5CE-ACBD-49C5-8582-010A26559740
:END:

Followed [[https://dvc.org/doc/install/completion?tab=Zsh][instructions for zsh]]. Worked.

** Turn notebook into to Python modules - Debugging
:PROPERTIES:
:CUSTOM_ID: h-17CACCE2-673E-4178-BFF8-195A2AA7E62A
:END:

In my case, running the modules got stuck.

#+begin_src shell
python src/data_preprocess.py --params params.yaml # OK
python src/data_load.py --params params.yaml       # OK
python src/train.py --params params.yaml           # FAIL
python src/evaluate.py --params params.yaml
#+end_src

First change. Since I am on Apple M2, use legacy Adam optimizer in
~compile_model~. Actually, TensorFlow did fallback automatically in first trial
run, but printed a warning.

#+begin_src python
# # default
# optimizer = keras.optimizers.Adam(learning_rate=MODEL_LEARNING_RATE) #Adam, RMSprop or SGD
# Legacy needed for M1/M2
optimizer = keras.optimizers.legacy.Adam(learning_rate=MODEL_LEARNING_RATE)  # Adam, RMSprop or SGD
#+end_src

Second change. Run ~train.py~ fails when calling ~model.fit~ with error message.

#+begin_example
# # default
# optimizer = keras.optimizers.Adam(learning_rate=MODEL_LEARNING_RATE) #Adam, RMSprop or SGD
# Legacy needed for M1/M2
optimizer = keras.optimizers.legacy.Adam(learning_rate=MODEL_LEARNING_RATE)  # Adam, RMSprop or SGD
#+end_example

Now figure out how to PyCharm debug script file with ~--params param.yaml~ arguments in the IDEs.

~src/train.py --params params.yaml~ error-2023-06-18-a.

#+begin_example
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 471, 471, 4)       404

 max_pooling2d (MaxPooling2  (None, 235, 235, 4)       0
 D)

 dropout (Dropout)           (None, 235, 235, 4)       0

 conv2d_1 (Conv2D)           (None, 231, 231, 4)       404

 max_pooling2d_1 (MaxPoolin  (None, 115, 115, 4)       0
 g2D)

 dense (Dense)               (None, 115, 115, 8)       40

 dropout_1 (Dropout)         (None, 115, 115, 8)       0

 flatten (Flatten)           (None, 105800)            0

 dense_1 (Dense)             (None, 1)                 105801

=================================================================
Total params: 106649 (416.60 KB)
Trainable params: 106649 (416.60 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Traceback (most recent call last):
  File "/Users/wasmer/src/github.com/irratzo/forks/dtc-workshop-pycharm/src/train.py", line 120, in <module>
    estimator = train_estimator(model)
  File "/Users/wasmer/src/github.com/irratzo/forks/dtc-workshop-pycharm/src/train.py", line 66, in train_estimator
    estimator = model.fit(X_train, y_train,
  File "/Users/wasmer/src/github.com/irratzo/forks/dtc-workshop-pycharm/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/Users/wasmer/src/github.com/irratzo/forks/dtc-workshop-pycharm/venv/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py", line 56, in _SatisfiesTypeConstraint
    raise TypeError(
TypeError: Value passed to parameter 'x' has DataType bool not in list of allowed values: bfloat16, float16, float32, float64, int8, int16, int32, int64, complex64, complex128
#+end_example
